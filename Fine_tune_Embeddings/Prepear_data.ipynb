{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import markdown\n",
    "import pandas as pd\n",
    "from chunking_evaluation.chunking import FixedTokenChunker, RecursiveTokenChunker, ClusterSemanticChunker, LLMSemanticChunker, KamradtModifiedChunker\n",
    "from chunking_evaluation import GeneralEvaluation, SyntheticEvaluation, BaseChunker\n",
    "from chunking_evaluation.utils import openai_token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Функция препроцессинга Markdown → TXT\n",
    " \n",
    " - Конвертирует MD в HTML, затем удаляет теги.\n",
    " - Убирает нумерацию абзацев и лишние разрывы строк.\n",
    " - Сохраняет чистый текст в `.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_md_to_txt(md_path: str, txt_path: str) -> None:\n",
    "    with open(md_path, 'r', encoding='utf-8') as f:\n",
    "        md = f.read()\n",
    "    html = markdown.markdown(md)\n",
    "    # Удаляем HTML-теги\n",
    "    text = re.sub(r'<[^>]+>', '', html)\n",
    "    # Убираем нумерацию в начале строк\n",
    "    text = re.sub(r'^\\s*\\d+[\\.\\)]\\s*', '', text, flags=re.MULTILINE)\n",
    "    # Нормализуем пробелы и переносы\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Инициализация и запуск `SyntheticEvaluation`\n",
    " \n",
    "Параметры:\n",
    "- `approximate_excerpts`: True — гибкое извлечение, False — точное.\n",
    "- `num_rounds`: число итераций генерации на документ.\n",
    "- `queries_per_corpus`: вопросов за раунд (`-1` = без ограничения)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed: /Users/sergey/Desktop/RAG_Legal_asist/RAG_Legal_assistant/Crowl4AI/court_decisions_combined.md → /Users/sergey/Desktop/RAG_Legal_asist/RAG_Legal_assistant/Crowl4AI/court_decisions_combined.txt\n"
     ]
    }
   ],
   "source": [
    "md_files = [\n",
    "    '/Users/sergey/Desktop/RAG_Legal_asist/RAG_Legal_assistant/Crowl4AI/court_decisions_combined.md',\n",
    "    # добавьте столько, сколько нужно\n",
    "]\n",
    "\n",
    "# Сначала преобразуем MD → TXT\n",
    "txt_files = []\n",
    "for md in md_files:\n",
    "    txt = md.replace('.md', '.txt')\n",
    "    preprocess_md_to_txt(md, txt)\n",
    "    txt_files.append(txt)\n",
    "    print(f\"Preprocessed: {md} → {txt}\")\n",
    "\n",
    "# Путь для итогового CSV\n",
    "output_csv = '/Users/sergey/Desktop/RAG_Legal_asist/RAG_Legal_assistant/Fine_tune_Embeddings/generated_queries.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key успешно загружен.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Загружаем переменные из файла .env\n",
    "load_dotenv()\n",
    "\n",
    "# Теперь переменные доступны в os.environ\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(\"OpenAI API Key успешно загружен.\")\n",
    "else:\n",
    "    print(\"Ошибка: OpenAI API Key не найден.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запустив эту ячейку, видим все используемые системные и пользовательские промпты, на которые опирается SyntheticEvaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== question_maker_system.txt ===\n",
      "\n",
      "You are an agent that generates questions from provided text. Your job is to generate a question and provide the relevant sections from the text as references.\n",
      "\n",
      "Instructions:\n",
      "1. For each provided text, generate a question that can be answered solely by the facts in the text.\n",
      "2. Extract all significant facts that answer the generated question.\n",
      "3. Format the response in JSON format with two fields:\n",
      "   - 'question': A question directly related to these facts, ensuring it can only be answered using the references provided.\n",
      "   - 'references': A list of all text sections that answer the generated question. These must be exact copies from the original text and should be whole sentences where possible.\n",
      "\n",
      "Notes: \n",
      "Make the question more specific.\n",
      "Do not ask a question about multiple topics. \n",
      "Do not ask a question with over 5 references.\n",
      "\n",
      "Example:\n",
      "\n",
      "Text: \"Experiment A: The temperature control test showed that at higher temperatures, the reaction rate increased significantly, resulting in quicker product formation. However, at extremely high temperatures, the reaction yield decreased due to the degradation of reactants.\n",
      "\n",
      "Experiment B: The pH sensitivity test revealed that the reaction is highly dependent on acidity, with optimal results at a pH of 7. Deviating from this pH level in either direction led to a substantial drop in yield.\n",
      "\n",
      "Experiment C: In the enzyme activity assay, it was found that the presence of a specific enzyme accelerated the reaction by a factor of 3. The absence of the enzyme, however, led to a sluggish reaction with an extended completion time.\n",
      "\n",
      "Experiment D: The light exposure trial demonstrated that UV light stimulated the reaction, making it complete in half the time compared to the absence of light. Conversely, prolonged light exposure led to unwanted side reactions that contaminated the final product.\"\n",
      "\n",
      "Response: {\n",
      "  'oath': \"I will not use the word 'and' in the question unless it is part of a proper noun. I will also make sure the question is concise.\",\n",
      "  'question': 'What experiments were done in this paper?',\n",
      "  'references': ['Experiment A: The temperature control test showed that at higher temperatures, the reaction rate increased significantly, resulting in quicker product formation.', 'Experiment B: The pH sensitivity test revealed that the reaction is highly dependent on acidity, with optimal results at a pH of 7.', 'Experiment C: In the enzyme activity assay, it was found that the presence of a specific enzyme accelerated the reaction by a factor of 3.', 'Experiment D: The light exposure trial demonstrated that UV light stimulated the reaction, making it complete in half the time compared to the absence of light.']\n",
      "}\n",
      "\n",
      "DO NOT USE THE WORD 'and' IN THE QUESTION UNLESS IT IS PART OF A PROPER NOUN. YOU MUST INCLUDE THE OATH ABOVE IN YOUR RESPONSE.\n",
      "YOU MUST ALSO NOT REPEAT A QUESTION THAT HAS ALREADY BEEN USED.\n",
      "\n",
      "=== question_maker_approx_system.txt ===\n",
      "\n",
      "You are an agent that generates questions from provided text. Your job is to generate a question and provide the relevant sections from the text as references.\n",
      "\n",
      "Instructions:\n",
      "1. For each provided text, generate a question that can be answered solely by the facts in the text.\n",
      "2. Extract all significant facts that answer the generated question.\n",
      "3. Format the response in JSON format with two fields:\n",
      "   - 'question': A question directly related to these facts, ensuring it can only be answered using the references provided.\n",
      "   - 'references': A list of JSON objects with the following fields: 'content': the text section that answers the question, 'start_chunk': the index of the start chunk, 'end_chunk': the index of the end chunk. These are inclusive indices.\n",
      "\n",
      "Notes: \n",
      "Make the question more specific.\n",
      "Do not ask a question about multiple topics. \n",
      "Do not ask a question with over 5 references.\n",
      "\n",
      "Example:\n",
      "\n",
      "Text: \"<start_chunk_0>Experiment A: The temperature control test showed that at higher temperatures, the reaction rate inc<end_chunk_0><start_chunk_1>reased significantly, resulting in quicker product formation. However, at extremely high temperature<end_chunk_1><start_chunk_2>s, the reaction yield decreased due to the degradation of reactants.\n",
      "\n",
      "Experiment B: The pH sensitivi<end_chunk_2><start_chunk_3>ty test revealed that the reaction is highly dependent on acidity, with optimal results at a pH of 7<end_chunk_3><start_chunk_4>. Deviating from this pH level in either direction led to a substantial drop in yield.\n",
      "\n",
      "Experiment C<end_chunk_4><start_chunk_5>: In the enzyme activity assay, it was found that the presence of a specific enzyme accelerated the <end_chunk_5><start_chunk_6>reaction by a factor of 3. The absence of the enzyme, however, led to a sluggish reaction with an ex<end_chunk_6><start_chunk_7>tended completion time.\n",
      "\n",
      "Experiment D: The light exposure trial demonstrated that UV light stimulate<end_chunk_7><start_chunk_8>d the reaction, making it complete in half the time compared to the absence of light. Conversely, pr<end_chunk_8><start_chunk_9>olonged light exposure led to unwanted side reactions that contaminated the final product.\n",
      "<end_chunk_9>\"\n",
      "\n",
      "Response: {\n",
      "  'oath': \"I will not use the word 'and' in the question unless it is part of a proper noun. I will also make sure the question is concise.\",\n",
      "  'question': 'What experiments were done in this paper?',\n",
      "  'references': [{\n",
      "    'content': 'Experiment A: The temperature control test showed that at higher temperatures, the reaction rate increased significantly, resulting in quicker product formation.',\n",
      "    'start_chunk': 0,\n",
      "    'end_chunk': 1,\n",
      "  }, {\n",
      "    'content': 'Experiment B: The pH sensitivity test revealed that the reaction is highly dependent on acidity, with optimal results at a pH of 7.',\n",
      "    'start_chunk': 2,\n",
      "    'end_chunk': 3,\n",
      "  }, {\n",
      "    'content': 'Experiment C: In the enzyme activity assay, it was found that the presence of a specific enzyme accelerated the reaction by a factor of 3.',\n",
      "    'start_chunk': 4,\n",
      "    'end_chunk': 6,\n",
      "  }, {\n",
      "    'content': 'Experiment D: The light exposure trial demonstrated that UV light stimulated the reaction, making it complete in half the time compared to the absence of light.',\n",
      "    'start_chunk': 7,\n",
      "    'end_chunk': 8,\n",
      "  }]\n",
      "}\n",
      "\n",
      "DO NOT USE THE WORD 'and' IN THE QUESTION UNLESS IT IS PART OF A PROPER NOUN. YOU MUST INCLUDE THE OATH ABOVE IN YOUR RESPONSE.\n",
      "YOU MUST ALSO NOT REPEAT A QUESTION THAT HAS ALREADY BEEN USED.\n",
      "\n",
      "=== question_maker_user.txt ===\n",
      "\n",
      "Text: {document}\n",
      "\n",
      "The following questions have already been used. Do not repeat them: {prev_questions_str}\n",
      "\n",
      "Do not repeat the above questions. Make your next question unique. Respond with references and a question in JSON. DO NOT USE THE WORD 'and' IN THE QUESTION UNLESS IT IS PART OF A PROPER NOUN.\n",
      "\n",
      "=== question_maker_approx_user.txt ===\n",
      "\n",
      "Text: {document}\n",
      "\n",
      "The following questions have already been used. Do not repeat them: {prev_questions_str}\n",
      "\n",
      "Do not repeat the above questions. Make your next question unique. Respond with references and a question in JSON. The references must contain the exact text that answers the question and the start_chunk and end_chunk. DO NOT USE THE WORD 'and' IN THE QUESTION UNLESS IT IS PART OF A PROPER NOUN.\n"
     ]
    }
   ],
   "source": [
    "from importlib import resources\n",
    "import os\n",
    "\n",
    "# Получаем путь к каталогу с промптами внутри пакета\n",
    "with resources.as_file(\n",
    "    resources.files('chunking_evaluation.evaluation_framework') / 'prompts'\n",
    ") as prompts_dir:\n",
    "    # Перечисляем все файлы промптов\n",
    "    for filename in [\n",
    "        'question_maker_system.txt',\n",
    "        'question_maker_approx_system.txt',\n",
    "        'question_maker_user.txt',\n",
    "        'question_maker_approx_user.txt'\n",
    "    ]:\n",
    "        file_path = os.path.join(prompts_dir, filename)\n",
    "        print(f\"\\n=== {filename} ===\\n\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Указываем путь(и) к вашим текстовым корпусам и CSV\n",
    "corpora_paths = [\n",
    "    '/Users/sergey/Desktop/RAG_Legal_asist/RAG_Legal_assistant/Fine_tune_Embeddings/court_decisions_combined.txt',\n",
    "]\n",
    "csv_path = '/Users/sergey/Desktop/RAG_Legal_asist/RAG_Legal_assistant/Fine_tune_Embeddings//generated_queries_and_excerpts.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вносим изменения в промпты, что бы генерация ответов и вопросов была на русском, так как мы хотим дообучить нашу модель на специфичном домене на русском"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RusSyntheticEvaluation import RusSyntheticEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Загрузим оригинальные английские промпты\n",
    "with resources.as_file(\n",
    "    resources.files('chunking_evaluation.evaluation_framework') / 'prompts'\n",
    ") as prompts_dir:\n",
    "    def load(name):\n",
    "        with open(os.path.join(prompts_dir, name), 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "    base_system     = load('question_maker_system.txt')\n",
    "    approx_system   = load('question_maker_approx_system.txt')\n",
    "    base_user       = load('question_maker_user.txt')\n",
    "    approx_user     = load('question_maker_approx_user.txt')\n",
    "\n",
    "# 2. Инструкция для генерации на русском\n",
    "rus_instruction = \"Please generate the question and the references in Russian.\\n\\n\"\n",
    "\n",
    "# 3. Собираем финальные тексты\n",
    "system_prompt        = rus_instruction + base_system\n",
    "approx_system_prompt = rus_instruction + approx_system\n",
    "\n",
    "# для точного режима\n",
    "user_prompt          = base_user   + \"\\n\\n\" + rus_instruction\n",
    "# для приближённого режима\n",
    "approx_user_prompt   = approx_user + \"\\n\\n\" + rus_instruction\n",
    "\n",
    "\n",
    "# Инициализируем конвейер без указания модели\n",
    "pipeline = RusSyntheticEvaluation(\n",
    "    corpora_paths,\n",
    "    csv_path,\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "# 5. Подменяем атрибуты экземпляра\n",
    "pipeline.question_maker_system_prompt        = system_prompt\n",
    "pipeline.question_maker_approx_system_prompt = approx_system_prompt\n",
    "pipeline.question_maker_user_prompt          = user_prompt\n",
    "pipeline.question_maker_approx_user_prompt   = approx_user_prompt\n",
    "\n",
    "# 6. Проверим, что в атрибутах действительно содержится русская инструкция\n",
    "assert pipeline.question_maker_system_prompt.startswith(rus_instruction)\n",
    "assert pipeline.question_maker_approx_system_prompt.startswith(rus_instruction)\n",
    "assert pipeline.question_maker_user_prompt.strip().endswith(rus_instruction.strip())\n",
    "assert pipeline.question_maker_approx_user_prompt.strip().endswith(rus_instruction.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## теперь промпты выглядят так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== system_prompt ===\n",
      "Please generate the question and the references in Russian.\n",
      "\n",
      "You are an agent that generates questions from provided text. Your job is to generate a question and provide the relevant sections from the ...\n",
      "\n",
      "=== approx_system_prompt ===\n",
      "Please generate the question and the references in Russian.\n",
      "\n",
      "You are an agent that generates questions from provided text. Your job is to generate a question and provide the relevant sections from the ...\n",
      "\n",
      "=== user_prompt ===\n",
      "Text: {document}\n",
      "\n",
      "The following questions have already been used. Do not repeat them: {prev_questions_str}\n",
      "\n",
      "Do not repeat the above questions. Make your next question unique. Respond with references a ...\n",
      "\n",
      "=== approx_user_prompt ===\n",
      "Text: {document}\n",
      "\n",
      "The following questions have already been used. Do not repeat them: {prev_questions_str}\n",
      "\n",
      "Do not repeat the above questions. Make your next question unique. Respond with references a ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== system_prompt ===\")\n",
    "print(pipeline.question_maker_system_prompt[:200], \"...\\n\")\n",
    "print(\"=== approx_system_prompt ===\")\n",
    "print(pipeline.question_maker_approx_system_prompt[:200], \"...\\n\")\n",
    "print(\"=== user_prompt ===\")\n",
    "print(pipeline.question_maker_user_prompt[:200], \"...\\n\")\n",
    "print(\"=== approx_user_prompt ===\")\n",
    "print(pipeline.question_maker_approx_user_prompt[:200], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     23\u001b[39m     os.remove(csv_path)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Сгенерировать очередную «пачку»\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[43mrun_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# Проверить, сколько уже накопилось\u001b[39;00m\n\u001b[32m     30\u001b[39m     df = pd.read_csv(csv_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mrun_batch\u001b[39m\u001b[34m(batch_size)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_batch\u001b[39m(batch_size=\u001b[32m20\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_queries_and_excerpts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapproximate_excerpts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqueries_per_corpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Удаляем «плохие» отрывки: из всех ссылок на вопрос остаются только те пары,\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# где минимальная семантическая похожесть вопроса и любого отрывка ≥ 0.36\u001b[39;00m\n\u001b[32m     10\u001b[39m     pipeline.filter_poor_excerpts(threshold=\u001b[32m0.36\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG_Legal_asist/RAG_Legal_assistant/.venv/lib/python3.12/site-packages/chunking_evaluation/evaluation_framework/synthetic_evaluation.py:223\u001b[39m, in \u001b[36mSyntheticEvaluation.generate_queries_and_excerpts\u001b[39m\u001b[34m(self, approximate_excerpts, num_rounds, queries_per_corpus)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m num_rounds == -\u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m rounds < num_rounds:\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m corpus_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.corpora_paths:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_corpus_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapprox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapproximate_excerpts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqueries_per_corpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     rounds += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG_Legal_asist/RAG_Legal_assistant/Fine_tune_Embeddings/RusSyntheticEvaluation.py:30\u001b[39m, in \u001b[36mRusSyntheticEvaluation._generate_corpus_questions\u001b[39m\u001b[34m(self, corpus_id, approx, n)\u001b[39m\n\u001b[32m     25\u001b[39m prev = \u001b[38;5;28mself\u001b[39m.synth_questions_df[\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mself\u001b[39m.synth_questions_df[\u001b[33m'\u001b[39m\u001b[33mcorpus_id\u001b[39m\u001b[33m'\u001b[39m] == corpus_id\n\u001b[32m     27\u001b[39m ][\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m approx:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     q, refs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_question_and_approx_references\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     32\u001b[39m     q, refs = \u001b[38;5;28mself\u001b[39m._extract_question_and_references(corpus, \u001b[32m4000\u001b[39m, prev)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG_Legal_asist/RAG_Legal_assistant/.venv/lib/python3.12/site-packages/chunking_evaluation/evaluation_framework/synthetic_evaluation.py:82\u001b[39m, in \u001b[36mSyntheticEvaluation._extract_question_and_approx_references\u001b[39m\u001b[34m(self, corpus, document_length, prev_questions)\u001b[39m\n\u001b[32m     78\u001b[39m     prev_questions_str = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     80\u001b[39m tagged_text, tag_indexes = \u001b[38;5;28mself\u001b[39m._tag_text(document)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m completion = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4-turbo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_object\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m600\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquestion_maker_approx_system_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquestion_maker_approx_user_prompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{document}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagged_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{prev_questions_str}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_questions_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m json_response = json.loads(completion.choices[\u001b[32m0\u001b[39m].message.content)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG_Legal_asist/RAG_Legal_assistant/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG_Legal_asist/RAG_Legal_assistant/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG_Legal_asist/RAG_Legal_assistant/.venv/lib/python3.12/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG_Legal_asist/RAG_Legal_assistant/.venv/lib/python3.12/site-packages/openai/_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m             err.response.read()\n\u001b[32m   1033\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# Функция для одной «пачки» генерации + фильтрации\n",
    "def run_batch(batch_size=20):\n",
    "    pipeline.generate_queries_and_excerpts(\n",
    "        approximate_excerpts=True,\n",
    "        num_rounds=1,\n",
    "        queries_per_corpus=batch_size\n",
    "    )\n",
    "    # Удаляем «плохие» отрывки: из всех ссылок на вопрос остаются только те пары,\n",
    "    # где минимальная семантическая похожесть вопроса и любого отрывка ≥ 0.36\n",
    "    pipeline.filter_poor_excerpts(threshold=0.36)\n",
    "\n",
    "    # Устраняем дубли и слишком схожие вопросы:\n",
    "    # сначала убираем точные копии, затем жадным алгоритмом сбрасываем\n",
    "    # все вопросы с косинусным сходством эмбеддингов > 0.7\n",
    "    pipeline.filter_duplicates(threshold=0.7)\n",
    "\n",
    "# Цикл до 200 вопросов\n",
    "TARGET = 200\n",
    "BATCH  = 20\n",
    "\n",
    "# Очистим предыдущий CSV (если нужно)\n",
    "if os.path.exists(csv_path):\n",
    "    os.remove(csv_path)\n",
    "\n",
    "while True:\n",
    "    # Сгенерировать очередную «пачку»\n",
    "    run_batch(batch_size=BATCH)\n",
    "    \n",
    "    # Проверить, сколько уже накопилось\n",
    "    df = pd.read_csv(csv_path)\n",
    "    cnt = len(df)\n",
    "    print(f\"Накоплено вопросов: {cnt}\")\n",
    "    \n",
    "    if cnt >= TARGET:\n",
    "        break\n",
    "\n",
    "print(f\"Готово! Всего сгенерировано и отфильтровано: {cnt} вопросов.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'generated_queries_and_excerpts.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m csv_output = \u001b[33m'\u001b[39m\u001b[33mdecoded_queries_Chunking_Evaluation.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 1) Загружаем\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 2) Функция декодирования\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_references\u001b[39m(ref_str):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG_Legal_asist/RAG_Legal_assistant/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG_Legal_asist/RAG_Legal_assistant/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG_Legal_asist/RAG_Legal_assistant/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG_Legal_asist/RAG_Legal_assistant/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/RAG_Legal_asist/RAG_Legal_assistant/.venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'generated_queries_and_excerpts.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Замените на реальный путь к вашему CSV\n",
    "csv_input = 'generated_queries_and_excerpts.csv'\n",
    "csv_output = 'decoded_queries_Chunking_Evaluation.csv'\n",
    "\n",
    "# 1) Загружаем\n",
    "df = pd.read_csv(csv_input, dtype=str)\n",
    "\n",
    "# 2) Функция декодирования\n",
    "def decode_references(ref_str):\n",
    "    try:\n",
    "        data = json.loads(ref_str)\n",
    "        return json.dumps(data, ensure_ascii=False)\n",
    "    except json.JSONDecodeError:\n",
    "        # fallback: прямой unicode-escape\n",
    "        return ref_str.encode('utf-8').decode('unicode_escape')\n",
    "\n",
    "# 3) Применяем к столбцу\n",
    "df['references'] = df['references'].apply(decode_references)\n",
    "\n",
    "# 4) Сохраняем результат\n",
    "df.to_csv(csv_output, index=False, encoding='utf-8-sig')\n",
    "# df.to_csv(csv_output, index=False)\n",
    "\n",
    "print(f\"Decoded table saved to {csv_output}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>references</th>\n",
       "      <th>corpus_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Какие требования истец предъявил в суде для вз...</td>\n",
       "      <td>[{\"content\": \"использованием фотоотчетов.\\nИст...</td>\n",
       "      <td>/Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Каковы были основания для восстановления срока...</td>\n",
       "      <td>[{\"content\": \"материалы дела и доводы, приведе...</td>\n",
       "      <td>/Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Каковы были основания суда для отказа ИП Безма...</td>\n",
       "      <td>[{\"content\": \"осквы от 17.10.2024 г., которым ...</td>\n",
       "      <td>/Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Какие юридические действия были предприняты фи...</td>\n",
       "      <td>[{\"content\": \"собственности на указанный объек...</td>\n",
       "      <td>/Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Какие документы представила истец Л.И. Атаманк...</td>\n",
       "      <td>[{\"content\": \"обоснование взыскания расходов н...</td>\n",
       "      <td>/Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Какие документы необходимо иметь при явке в су...</td>\n",
       "      <td>[{\"content\": \"7.\\n\\nПри явке в судебное заседа...</td>\n",
       "      <td>/Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Что указал истец о подписанных документах по д...</td>\n",
       "      <td>[{\"content\": \"Васкул В.Л. является застрахован...</td>\n",
       "      <td>/Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Какова сумма общей задолженности и неустойки, ...</td>\n",
       "      <td>[{\"content\": \"\\\"Федеральная сетевая компания Е...</td>\n",
       "      <td>/Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Какие действия по ремонту квартиры были выполн...</td>\n",
       "      <td>[{\"content\": \"2018 года по март 2021 года собс...</td>\n",
       "      <td>/Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>На каком основании суд апелляционной инстанции...</td>\n",
       "      <td>[{\"content\": \"деле, судьей единолично.\\nПровер...</td>\n",
       "      <td>/Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0    Какие требования истец предъявил в суде для вз...   \n",
       "1    Каковы были основания для восстановления срока...   \n",
       "2    Каковы были основания суда для отказа ИП Безма...   \n",
       "3    Какие юридические действия были предприняты фи...   \n",
       "4    Какие документы представила истец Л.И. Атаманк...   \n",
       "..                                                 ...   \n",
       "179  Какие документы необходимо иметь при явке в су...   \n",
       "180  Что указал истец о подписанных документах по д...   \n",
       "181  Какова сумма общей задолженности и неустойки, ...   \n",
       "182  Какие действия по ремонту квартиры были выполн...   \n",
       "183  На каком основании суд апелляционной инстанции...   \n",
       "\n",
       "                                            references  \\\n",
       "0    [{\"content\": \"использованием фотоотчетов.\\nИст...   \n",
       "1    [{\"content\": \"материалы дела и доводы, приведе...   \n",
       "2    [{\"content\": \"осквы от 17.10.2024 г., которым ...   \n",
       "3    [{\"content\": \"собственности на указанный объек...   \n",
       "4    [{\"content\": \"обоснование взыскания расходов н...   \n",
       "..                                                 ...   \n",
       "179  [{\"content\": \"7.\\n\\nПри явке в судебное заседа...   \n",
       "180  [{\"content\": \"Васкул В.Л. является застрахован...   \n",
       "181  [{\"content\": \"\\\"Федеральная сетевая компания Е...   \n",
       "182  [{\"content\": \"2018 года по март 2021 года собс...   \n",
       "183  [{\"content\": \"деле, судьей единолично.\\nПровер...   \n",
       "\n",
       "                                             corpus_id  \n",
       "0    /Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...  \n",
       "1    /Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...  \n",
       "2    /Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...  \n",
       "3    /Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...  \n",
       "4    /Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...  \n",
       "..                                                 ...  \n",
       "179  /Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...  \n",
       "180  /Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...  \n",
       "181  /Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...  \n",
       "182  /Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...  \n",
       "183  /Users/sergey/Desktop/RAG_Legal_asist/RAG_Lega...  \n",
       "\n",
       "[184 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"content\": \"использованием фотоотчетов.\\\\nИстец просит суд: взыскать солидарно с ответчиков фио и Ивановой М.И. в пользу истца неосновательное обогащение полученное ответчиками в результате понесенных истцом затрат на выполнение ремонта принадлежащей ответчикам спорной квартиры - за проектную документацию в размере сумма, оплату ремонтных работ в сумме сумма и закупку строительных и отделочных материалов, инструментов, оборудования в размере сумма необходимых для ремонта квартиры принадлежащей ответчикам, а всего сумма; неосновательное обогащение полученное ответчиками в результате понесенных истцом затрат связанных с оплатой им коммунальных услуг, за отопление и содержание принадлежащей ответчикам квартиры за период с 01.02.2021 по 31.11.2021 в сумме сумма\\\\nСуд \", \"start_index\": 52294, \"end_index\": 53052}]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['references'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Пример загрузки вашего DataFrame\n",
    "df = pd.read_csv(\"/Users/sergey/Desktop/RAG_Legal_asist/RAG_Legal_assistant/Fine_tune_Embeddings/decoded_queries_Chunking_Evaluation .csv\")  # если он из файла\n",
    "\n",
    "def extract_contents_only(ref_string):\n",
    "    try:\n",
    "        # Преобразуем строку в список словарей\n",
    "        refs = json.loads(ref_string)\n",
    "        # Извлекаем только тексты\n",
    "        return [item['content'] for item in refs if 'content' in item]\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return []\n",
    "\n",
    "# Применяем функцию к каждой строке\n",
    "df['references'] = df['references'].apply(extract_contents_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['использованием фотоотчетов.\\nИстец просит суд: взыскать солидарно с ответчиков фио и Ивановой М.И. в пользу истца неосновательное обогащение полученное ответчиками в результате понесенных истцом затрат на выполнение ремонта принадлежащей ответчикам спорной квартиры - за проектную документацию в размере сумма, оплату ремонтных работ в сумме сумма и закупку строительных и отделочных материалов, инструментов, оборудования в размере сумма необходимых для ремонта квартиры принадлежащей ответчикам, а всего сумма; неосновательное обогащение полученное ответчиками в результате понесенных истцом затрат связанных с оплатой им коммунальных услуг, за отопление и содержание принадлежащей ответчикам квартиры за период с 01.02.2021 по 31.11.2021 в сумме сумма\\nСуд ']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['references'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output = 'decoded_queries_for_finetune.csv'\n",
    "\n",
    "# 4) Сохраняем результат\n",
    "df.to_csv(csv_output, index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
